# MLP-Multi-Layer-Perceptron (MNIST)

By Leon Davis.

Este proyecto implementa una red neuronal perceptr√≥n multicapa (MLP) entrenada para reconocer d√≠gitos del 0 al 9 utilizando el dataset MNIST. Se ha desarrollado en C++ usando CMake como sistema de construcci√≥n y OpenCV para el manejo de im√°genes.

## üîß Requisitos

* CMake >= 3.10
* OpenCV >= 4.0

Aseg√∫rate de tener instalados los requisitos antes de compilar.

## üöÄ Instalaci√≥n

Clona el repositorio y entra en la carpeta del proyecto:

```bash
git clone https://github.com/LeonDavisCoropuna/MLP-Multi-Layer-Perceptron.git
cd MLP-Multi-Layer-Perceptron
```

Dale permisos de ejecuci√≥n al script principal:

```bash
chmod +x run.sh
```

Ejecuta el script para compilar y correr:

```bash
./run.sh
```

## üìÅ Estructura del proyecto

```bash
tree --dirsfirst -I 'mnist_data|build|venv'
.
‚îú‚îÄ‚îÄ images
‚îÇ   ‚îú‚îÄ‚îÄ and_tanh_sigmoid_console.png
‚îÇ   ‚îú‚îÄ‚îÄ or_tanh_sigmoid_console.png
‚îÇ   ‚îú‚îÄ‚îÄ result_minst_20_epochs.png
‚îÇ   ‚îú‚îÄ‚îÄ result_minst_30_epochs.png
‚îÇ   ‚îú‚îÄ‚îÄ xor_relu_relu_console.png
‚îÇ   ‚îú‚îÄ‚îÄ xor_relu_relu.png
‚îÇ   ‚îú‚îÄ‚îÄ xor_sigmoid_sigmoid_console.png
‚îÇ   ‚îú‚îÄ‚îÄ xor_sigmoid_sigmoid.png
‚îÇ   ‚îú‚îÄ‚îÄ xor_tanh_sigmoid_console.png
‚îÇ   ‚îú‚îÄ‚îÄ xor_tanh_sigmoid.png
‚îÇ   ‚îú‚îÄ‚îÄ xor_tanh_tanh_console.png
‚îÇ   ‚îî‚îÄ‚îÄ xor_tanh_tanh.png
‚îú‚îÄ‚îÄ models
‚îÇ   ‚îú‚îÄ‚îÄ MLP.hpp
‚îÇ   ‚îú‚îÄ‚îÄ perceptron.hpp
‚îÇ   ‚îî‚îÄ‚îÄ singleLayerPerceptron.hpp
‚îú‚îÄ‚îÄ numbers
‚îÇ   ‚îú‚îÄ‚îÄ Captura desde 2025-05-19 16-56-22.png
‚îÇ   ‚îú‚îÄ‚îÄ Captura desde 2025-05-19 16-58-38.png
‚îÇ   ‚îú‚îÄ‚îÄ Captura desde 2025-05-19 17-06-35.png
‚îÇ   ‚îú‚îÄ‚îÄ ...
‚îÇ   ‚îú‚îÄ‚îÄ ...
‚îÇ   ‚îú‚îÄ‚îÄ ...
‚îÇ   ‚îú‚îÄ‚îÄ convert.py
‚îú‚îÄ‚îÄ save_models
‚îÇ   ‚îú‚îÄ‚îÄ minst_test.txt
‚îÇ   ‚îú‚îÄ‚îÄ minst_weights.txt
‚îú‚îÄ‚îÄ utils
‚îÇ   ‚îú‚îÄ‚îÄ activations.hpp
‚îÇ   ‚îú‚îÄ‚îÄ load_dataset.hpp
‚îÇ   ‚îú‚îÄ‚îÄ loss.hpp
‚îÇ   ‚îú‚îÄ‚îÄ optimizer.hpp
‚îÇ   ‚îî‚îÄ‚îÄ test_image.hpp
‚îú‚îÄ‚îÄ mnist_data
‚îÇ   ‚îú‚îÄ‚îÄ train
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ 0
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ 1
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ ...
‚îÇ   ‚îî‚îÄ‚îÄ test
‚îÇ       ‚îú‚îÄ‚îÄ 0
‚îÇ       ‚îú‚îÄ‚îÄ 1
‚îÇ       ‚îî‚îÄ‚îÄ ...
‚îú‚îÄ‚îÄ CMakeLists.txt
‚îú‚îÄ‚îÄ Lab3_MLP_Leon_Davis.pdf
‚îú‚îÄ‚îÄ LICENSE
‚îú‚îÄ‚îÄ main.cpp
‚îú‚îÄ‚îÄ README.md
‚îú‚îÄ‚îÄ README_XOR.md
‚îú‚îÄ‚îÄ run.sh
‚îú‚îÄ‚îÄ test.cpp
‚îú‚îÄ‚îÄ training_outputs.txt
‚îî‚îÄ‚îÄ xor.cpp

5 directories, 43 files
```

> **Nota:** La carpeta `mnist_data` contiene dos subcarpetas (`train` y `test`) con las im√°genes del dataset MNIST organizadas por d√≠gito.

## üß† Funcionalidades √∫tiles

### üîπ Cargar pesos preentrenados

Puedes cargar pesos guardados previamente utilizando:

```cpp
mlp.load_model_weights("save_models/minst_weights.txt");
```

### üîπ Predecir a partir de una imagen

Para predecir un d√≠gito a partir de una imagen personalizada:

```cpp
flatten_image_to_vector_and_predict("numbers/cinco_5.png", mlp);
```

Esto cargar√° la imagen `cinco_5.png`, la preprocesar√° y mostrar√° la predicci√≥n del modelo MLP entrenado.

Claro, continuemos con la secci√≥n **## Implementaci√≥n** explicando el c√≥digo del `MLP` (Multi-Layer Perceptron) paso a paso.

## üõ†Ô∏è Implementaci√≥n

A continuaci√≥n, se presenta una clase `MLP` que representa una red neuronal multicapa (Multi-Layer Perceptron) en C++. Esta implementaci√≥n se basa en la clase `SingleLayerPerceptron` y proporciona funcionalidades para entrenamiento, evaluaci√≥n, predicci√≥n y guardado/carga de pesos del modelo.

### üîß Atributos principales

```cpp
float learning_rate;
vector<int> num_layers;
int num_inputs;
vector<SingleLayerPerceptron *> layers;
vector<vector<float>> output_layers;
vector<ActivationFunction *> activations;
Loss *loss_function;
int last_output_size = -1;
Optimizer *optimizer;
```

* `learning_rate`: tasa de aprendizaje usada en la actualizaci√≥n de pesos.
* `num_layers`: vector que define la cantidad de neuronas por capa.
* `layers`: cada elemento representa una capa `SingleLayerPerceptron`.
* `activations`: funciones de activaci√≥n por capa.
* `loss_function`: funci√≥n de p√©rdida (e.g. MSE, CrossEntropy).
* `optimizer`: permite usar distintos optimizadores (SGD, Adam, etc.).
* `last_output_size`: usado para conectar capas din√°micamente.
* `output_layers`: almacena las salidas de cada capa tras un `forward`.

### üß± Constructores

#### Constructor con arquitectura definida:

```cpp
MLP(float _learning_rate, vector<int> _num_layers,
    vector<ActivationFunction *> _activations, Loss *_loss_function)
```

* Define la arquitectura completa de entrada.
* Instancia cada `SingleLayerPerceptron` con su funci√≥n de activaci√≥n correspondiente.

#### Constructor din√°mico:

```cpp
MLP(float _learning_rate, Optimizer *_optimizer)
```

* Permite construir la red din√°micamente usando `add_input_layer` y `add_layer`.

### ‚ûï M√©todos de construcci√≥n

#### A√±adir capa de entrada:

```cpp
void add_input_layer(int input_size, int num_neurons, ActivationFunction *activationFunction)
```

* Inicializa la red con la capa de entrada.

#### A√±adir capas ocultas:

```cpp
void add_layer(int num_neurons, ActivationFunction *activationFunction)
```

* A√±ade una capa oculta o de salida, usando la salida previa como entrada.

### üîÆ Predicci√≥n

#### `int predict(const vector<float> &input)`

* Aplica un `forward` y devuelve la clase predicha:

  * Binario: si solo hay una salida, umbral de 0.5.
  * Multiclase: √≠ndice de la salida con valor m√°s alto.

### üîÅ Forward propagation

```cpp
vector<float> forward(vector<float> batch_inputs)
```

* Calcula la salida de la red hacia adelante, almacenando cada salida intermedia.

### üéì Entrenamiento

```cpp
void train(int num_epochs, const vector<vector<float>> &X, const vector<float> &Y)
```

* Entrena la red:

  1. Para cada epoch, se recorre todo el dataset.
  2. Se realiza `forward` para cada muestra.
  3. Se calcula la p√©rdida (loss).
  4. Se realiza backpropagation:

     * Se propaga el error desde la √∫ltima capa (`backward_output_layer`) hacia las ocultas (`backward_hidden_layer`).
     * Se actualizan los pesos (`update_weights`).
  5. Imprime la p√©rdida y precisi√≥n por epoch.


### üìä Evaluaci√≥n

```cpp
float evaluate(const vector<vector<float>> &X_test, const vector<float> &Y_test)
```

* Eval√∫a la red sobre datos de prueba.
* Calcula e imprime la precisi√≥n final.


### üíæ Guardado de pesos

```cpp
void save_model_weights(const std::string &filename)
```

* Guarda los pesos de cada neurona en un archivo de texto plano.
* Formato legible, √∫til para an√°lisis o reproducibilidad.

### üìÇ Carga de pesos

```cpp
void load_model_weights(const std::string &filename)
```

* Carga los pesos desde un archivo previamente guardado.
* Verifica consistencia en cantidad de capas y neuronas.

### üßπ Destructor

```cpp
~MLP()
```

* Libera memoria reservada din√°micamente para las capas, funciones de activaci√≥n y p√©rdida.

## Utilidades

### üîß **Clases de Funciones de Activaci√≥n**

Cada clase hereda de `ActivationFunction`, una interfaz base que define el comportamiento general de las funciones de activaci√≥n: `activate()`, `derivative()`, `initialize_weights()` y `activate_vector()` (opcional).

#### üî∏ `ReLU` (Rectified Linear Unit)

* **`activate(x)`**: devuelve `x` si es positivo, `0` si es negativo.
* **`derivative(x)`**: devuelve `1` si `x > 0`, `0` en otro caso.
* **`initialize_weights()`**: inicializa los pesos con distribuci√≥n uniforme escalada con `sqrt(2 / num_inputs)` (He initialization).

#### üî∏ `Tanh`

* **`activate(x)`**: retorna `tanh(x)`, que transforma el valor a un rango entre -1 y 1.
* **`derivative(x)`**: `1 - tanh(x)^2`, que es la derivada de `tanh`.
* **`initialize_weights()`**: usa una distribuci√≥n uniforme escalada por `sqrt(1 / num_inputs)` (Xavier initialization).

#### üî∏ `Sigmoid`

* **`activate(x)`**: funci√≥n log√≠stica: `1 / (1 + exp(-x))`, salida entre 0 y 1.
* **`derivative(x)`**: `sigmoid(x) * (1 - sigmoid(x))`.
* **`initialize_weights()`**: igual que `Tanh`, usa Xavier initialization.

#### üî∏ `Softmax`

* Se usa generalmente en la **capa de salida** para clasificaci√≥n multiclase.
* **`activate(x)`**: no aplica individualmente a un escalar. Aqu√≠ solo est√° por compatibilidad.
* **`activate_vector(vector)`**: aplica softmax al vector completo:

  1. Resta el valor m√°ximo (estabilidad num√©rica).
  2. Aplica `exp`.
  3. Normaliza dividiendo entre la suma.
* **`derivative(x)`**: no se usa directamente.
* **`requires_special_output_gradient()`**: devuelve `true`, indicando que debe manejarse especialmente (como en `cross-entropy + softmax`).


### üì¶ **Carga de Dataset: `load_dataset()`**

Esta funci√≥n carga un conjunto de im√°genes PNG en escala de grises desde una carpeta:

1. **Detecta etiquetas** en el nombre del archivo con regex (`label_N.png`).
2. **Lee im√°genes** con OpenCV (`cv::imread`).
3. **Normaliza** cada p√≠xel a rango `[0, 1]`.
4. **Aplana** la imagen a un vector 1D.
5. Devuelve un `pair`: lista de im√°genes (`X`) y etiquetas (`Y`).

### üîç **Predicci√≥n desde Imagen: `flatten_image_to_vector_and_predict()`**

Funci√≥n √∫til para probar el modelo con una imagen individual.

1. Lee la imagen en escala de grises.
2. La redimensiona a 28√ó28 (como MNIST).
3. La binariza (blanco y negro) con un **umbral** de 128.
4. Muestra la matriz 28√ó28 por consola.
5. Aplana y normaliza la imagen.
6. Llama al m√©todo `predict()` del MLP y muestra el resultado.

### ‚öôÔ∏è **Optimizadores: SGD y Adam**

Ambos heredan de la clase abstracta `Optimizer`.

#### üî∏ `SGD` (Stochastic Gradient Descent)

* Actualiza los pesos con la f√≥rmula est√°ndar:

  $$
  w_i = w_i - \eta \cdot \frac{\partial L}{\partial w_i}
  $$

#### üî∏ `Adam` (Adaptive Moment Estimation)

* Mantiene **promedios m√≥viles** de los gradientes y sus cuadrados:

  * `m_weights`: primer momento (media).
  * `v_weights`: segundo momento (varianza).
* Usa correcci√≥n de sesgo (`bias correction`) para ajustar `m_hat` y `v_hat`.
* Mejora la estabilidad del entrenamiento especialmente con tasas de aprendizaje m√°s grandes.

## Entrenamiento

Este programa en C++ carga un modelo de red neuronal multicapa (MLP) para clasificar im√°genes del conjunto de datos MNIST. A continuaci√≥n se explica paso a paso cada parte del c√≥digo:

### 1. Inclusi√≥n de cabeceras

```cpp
#include "models/MLP.hpp"
#include "utils/load_dataset.hpp"
#include <chrono>
````

Se incluyen:

* El modelo `MLP` (una red neuronal feedforward).
* La utilidad `load_dataset.hpp` para cargar datos de imagen.
* `<chrono>` para medir el tiempo de entrenamiento.

### 2. Inicializaci√≥n del generador aleatorio

```cpp
mt19937 Perceptron::gen(32);
```

Se inicializa un generador de n√∫meros aleatorios con semilla 32, usado probablemente en la inicializaci√≥n de pesos en la clase `Perceptron`.

### 3. Funci√≥n principal `main()`

#### Carga de los datos

```cpp
auto train_data = load_dataset("mnist_data/saved_images/train");
auto test_data = load_dataset("mnist_data/saved_images/test");
```

* Se cargan las im√°genes de entrenamiento y prueba desde las carpetas locales.
* `train_data` y `test_data` son pares `std::pair<vector, vector>` con im√°genes y etiquetas.

```cpp
std::cout << "Cargadas " << train_data.first.size() << " im√°genes de entrenamiento." << std::endl;
std::cout << "Cargadas " << test_data.first.size() << " im√°genes de prueba." << std::endl;
```

Se imprime cu√°ntas im√°genes se han cargado.

### 4. Configuraci√≥n del modelo MLP

```cpp
float learning_rate = 0.001f;
Optimizer *sgd = new SGD(learning_rate);
MLP mlp(learning_rate, sgd);
```

* Se define la tasa de aprendizaje.
* Se instancia un optimizador `SGD`.
* Se crea un modelo MLP con ese optimizador.

```cpp
mlp.add_input_layer(784, 128, new ReLU());
mlp.add_layer(64, new ReLU());
mlp.add_layer(10, new Softmax());
mlp.set_loss(new CrossEntropyLoss());
```

* Se define la arquitectura de la red:

  * Capa de entrada: 784 neuronas (28x28 px), 128 de salida con activaci√≥n ReLU.
  * Capa oculta: 64 neuronas con ReLU.
  * Capa de salida: 10 neuronas (d√≠gitos del 0 al 9) con Softmax.
* Se define la funci√≥n de p√©rdida como entrop√≠a cruzada.

### 5. Medici√≥n del tiempo de entrenamiento

```cpp
auto start_time = std::chrono::high_resolution_clock::now();
```

Se inicia el conteo del tiempo.

```cpp
mlp.train(30, train_data.first, train_data.second);
//mlp.load_model_weights("save_models/minst_weights.txt");
```

* En el entrenamiento se indican las epocas (30) junto con los datos de entrenamiento y evaluacion
* En caso de haber entrenado el modelo anteriormente ya no es necesario entrenar desde cero, se cargan pesos preentrenados.

```cpp
auto end_time = std::chrono::high_resolution_clock::now();
std::chrono::duration<double> duration = end_time - start_time;
std::cout << "Tiempo total de entrenamiento: " << duration.count() << " segundos" << std::endl;
```

Se calcula e imprime el tiempo total que tom√≥ cargar el modelo (o entrenarlo, si se usa la l√≠nea comentada). Luego se prueba el modelo en el conjunto de evaluaci√≥n y se imprime el accuracy.

* Ejemplo de salida en el entrenamiento

![Salida 1](images/result_minst_30_epochs.png)

### Notas adicionales

Tambi√©n se muestran l√≠neas comentadas para:

* Entrenar el modelo.
* Evaluar todo el conjunto de prueba.
* Guardar los pesos del modelo entrenado.

Estas funcionalidades est√°n disponibles y pueden activarse f√°cilmente.

## Ejemplos de salidas

![Ejemplo 1](images/example-output-2.png)

![Ejemplo 2](images/example-output-3.png)

![Ejemplo 2](images/example-output-4.png)
