# MLP-Multi-Layer-Perceptron (MNIST) - L2 & Dropout

By Leon Davis.

Este proyecto implementa una red neuronal perceptr贸n multicapa (MLP) entrenada para reconocer d铆gitos del 0 al 9 utilizando el dataset MNIST. Se ha desarrollado en C++ usando CMake como sistema de construcci贸n y OpenCV para el manejo de im谩genes.

##  Requisitos

* CMake >= 3.10
* OpenCV >= 4.0

Aseg煤rate de tener instalados los requisitos antes de compilar.

## Instalaci贸n

Clona el repositorio y entra en la carpeta del proyecto:

```bash
git clone https://github.com/LeonDavisCoropuna/MLP-Multi-Layer-Perceptron.git
cd MLP-Multi-Layer-Perceptron
```

Dale permisos de ejecuci贸n al script principal:

```bash
chmod +x run.sh
```

Ejecuta el script para compilar y correr:

```bash
./run.sh main
```

## Evaluaci贸n
Se implementaron 5 casos con la misma arquitectura de 784x64x32x10:
- Arquitectura normal.
- Arquitectura con dropout de 0.2 entre capas
- Arquitectura con weight decay de 0.01
- Arquitectura con dropout 0.2 y weight decay 0.0005
- Arquitectura con dropout 0.2 y weight decay 0.01

### Caso 1 (sin drop ni weight decay):
```cpp
float learning_rate = 0.001f;
float wd = 0;

Optimizer *adam = new Adam(learning_rate, 0);
MLP mlp(learning_rate, adam);

mlp.add_layer(new DenseLayer(784, 64, new ReLU(), adam));
mlp.add_layer(new DenseLayer(64, 32, new ReLU(), adam));
mlp.add_layer(new DenseLayer(32, 10, new Softmax(), adam));
mlp.set_loss(new CrossEntropyLoss());
```

![alt text](images/784x64x32x10-acc.png)
![alt text](images/784x64x32x10-loss.png)

Mejor Tess Accuracy en Epoch 19:
  - Train Loss     : 0.0133
  - Train Accuracy : 99.57%
  - Test Loss      : 0.1286
  - Test Accuracy  : 97.40%

El modelo logr贸 una alta precisi贸n en el conjunto de entrenamiento (99.57%), lo que indica un ajuste casi perfecto a los datos de entrenamiento. Sin embargo, con la precisi贸n de test (97.40%) se ve  un ligero sobreajuste. Sin embargo, el sobreajuste empieza desde el epoch 2 y desde all铆 la mejora del test no es tan evidente mientras que la mejora del train sigue aumentando sin parar.


### Caso 2 (solo drop)

```cpp
float learning_rate = 0.001f;
float wd = 0;

Optimizer *adam = new Adam(learning_rate, wd);
MLP mlp(learning_rate, adam);

mlp.add_layer(new DenseLayer(784, 64, new ReLU(), adam));
mlp.add_layer(new DropoutLayer(0.2));
mlp.add_layer(new DenseLayer(64, 32, new ReLU(), adam));
mlp.add_layer(new DropoutLayer(0.2));
mlp.add_layer(new DenseLayer(32, 10, new Softmax(), adam));
mlp.set_loss(new CrossEntropyLoss());
```

![alt text](images/drop02-arch784x64x32x10-acc.png)
![alt text](images/drop02-arch784x64x32x10-loss.png)

Mejor Tess Accuracy en Epoch 16:
  - Train Loss     : 0.1026
  - Train Accuracy : 96.92%
  - Test Loss      : 0.0952
  - Test Accuracy  : 97.43%

El mejor modelo, la inclusi贸n de dropout ayud贸 a reducir el sobreajuste, como lo evidencia el aumento en la precisi贸n de prueba (97.43%) con respecto al caso base, aunque con una menor precisi贸n de entrenamiento.

### Caso 3 (solo weight decay)

```cpp
float learning_rate = 0.001f;
float wd = 0.0005f;

Optimizer *adam = new Adam(learning_rate, wd);
MLP mlp(learning_rate, adam);

mlp.add_layer(new DenseLayer(784, 64, new ReLU(), adam));
mlp.add_layer(new DenseLayer(64, 32, new ReLU(), adam));
mlp.add_layer(new DenseLayer(32, 10, new Softmax(), adam));
mlp.set_loss(new CrossEntropyLoss());
```

![alt text](images/wd-001-arch784x64x32x10-acc.png)
![alt text](images/wd-001-arch784x64x32x10-loss.png)

Mejor Tess Accuracy en Epoch 19:
  - Train Loss     : 0.2403
  - Train Accuracy : 93.59%
  - Test Loss      : 0.2748
  - Test Accuracy  : 91.65%

El weight decay aplicado fue probablemente demasiado fuerte, lo que result贸 en una reducci贸n significativa del rendimiento tanto en entrenamiento como en prueba (Train: 93.59%, Test: 91.65%). Este caso muestra c贸mo una penalizaci贸n excesiva puede dificultar el aprendizaje.

### Caso 4 (drop 0.2 y wd 0.01)

```cpp
float learning_rate = 0.001f;
float wd = 0.01f;

Optimizer *adam = new Adam(learning_rate, wd);
MLP mlp(learning_rate, adam);

mlp.add_layer(new DenseLayer(784, 64, new ReLU(), adam));
mlp.add_layer(new DenseLayer(64, 32, new ReLU(), adam));
mlp.add_layer(new DenseLayer(32, 10, new Softmax(), adam));
mlp.set_loss(new CrossEntropyLoss());
```

![alt text](images/drop02-wd-0.01-arch784x64x32x10-acc.png)
![alt text](images/drop02-wd-001-arch784x64x32x10-loss.png)

Mejor Tess Accuracy en Epoch 9:
  - Train Loss     : 0.3758
  - Train Accuracy : 89.44%
  - Test Loss      : 0.2678
  - Test Accuracy  : 92.37%

Este enfoque combin贸 dos t茅cnicas de regularizaci贸n, pero el valor alto de weight decay nuevamente afect贸 negativamente el aprendizaje. Aunque el modelo evit贸 el sobreajuste (Train: 89.44%, Test: 92.37%), su capacidad de alcanzar altos niveles de precisi贸n fue limitada. Este experimento confirma que una penalizaci贸n muy fuerte no se compensa con dropout.

### Caso 5 (drop 0.2 y wd 0.0005)

```cpp
float learning_rate = 0.001f;
float wd = 0.0005f;

Optimizer *adam = new Adam(learning_rate, wd);
MLP mlp(learning_rate, adam);

mlp.add_layer(new DenseLayer(784, 64, new ReLU(), adam));
mlp.add_layer(new DenseLayer(64, 32, new ReLU(), adam));
mlp.add_layer(new DenseLayer(32, 10, new Softmax(), adam));
mlp.set_loss(new CrossEntropyLoss());
```

![alt text](images/drop02-wd-0005-arch784x64x32x10-acc.png)
![alt text](images/drop02-wd-0005-arch784x64x32x10-acc.png)

Mejor Tess Accuracy en Epoch 19:
  - Train Loss     : 0.1456
  - Train Accuracy : 95.72%
  - Test Loss      : 0.0866
  - Test Accuracy  : 97.35%

Esta combinaci贸n equilibrada produjo uno de los mejores resultados generales, con alta precisi贸n en test (97.35%) y buena capacidad de generalizaci贸n. El modelo evit贸 el sobreajuste severo y logr贸 un rendimiento comparable al mejor caso (caso 2), pero con una regularizaci贸n m谩s controlada. Es una configuraci贸n 贸ptima entre complejidad y generalizaci贸n.

### Recopilaci贸n
#### Gr谩fica de de loss solo en el conjunto de tess

![alt text](images/all-test-loss.png)

Directamente aqu铆 se observa que aplicar un L2 con valor de 0.01 aumenta la perdida de los modelos al ser un valor muy grande, es mucho mejor un valor mas peque帽o como 0.0005. Tambi茅n es notorio que la curva del modelo sin L2 ni dropout empieza bien, pero conforme aumentan los epochs cada vez aumenta su perdida indicando que el sobreajuste se har谩 mayor a m谩s epochs.

#### Gr谩fica de accuracy solo en el conjunto de tess

![alt text](images/all-test-acc.png)
Aqu铆 se observa que al igual que en loss los peores modelos son los que tienen un L2 de 0.01. El accuracy de los otros modelos es muy bueno, y el que reslta m谩s es el que implementa solo dropout alcanzando el mayor accuracy.

## Conclusiones

### 1. El uso de dropout mejor贸 la generalizaci贸n del modelo sin afectar significativamente la precisi贸n.
En el segundo caso (solo dropout), se observ贸 una mayor regularizaci贸n respecto al modelo base. Aunque la precisi贸n en entrenamiento disminuy贸 ligeramente (96.92% frente a 99.57%), la precisi贸n en test fue incluso superior (97.43% vs. 97.40%), con menor test loss. Esto indica que el dropout ayud贸 a prevenir el sobreajuste.

### 2. El weight decay por s铆 solo no fue suficiente y redujo notablemente la capacidad del modelo.
En el tercer caso (solo weight decay), la precisi贸n en test cay贸 a 91.65% y la p茅rdida fue mucho mayor que en los dem谩s casos demotrando que un alto valor de L2 (0.01) afectar铆a negativamente al modelo incluso a帽adiendo dropout.

### 3. La combinaci贸n de dropout y weight decay moderado logr贸 un buen equilibrio entre regularizaci贸n y rendimiento.
El quinto caso (dropout 0.2 + weight decay 0.0005) logr贸 un rendimiento muy competitivo con 97.35% de test accuracy y el menor test loss (0.0866) de todos los casos. Esta combinaci贸n favoreci贸 tanto la regularizaci贸n como la capacidad de aprendizaje, logrando un modelo robusto y eficaz. A diferencia del cuarto caso (dropout + wd 0.01), donde un wd m谩s alto redujo significativamente el rendimiento, esta configuraci贸n muestra que la sinergia entre t茅cnicas debe mantenerse en valores equilibrado

## Ver c贸digo en github:
La parte principal del c贸digo se encuentra en la carpeta models/ (MLP, layers) y en utils/ (optimizadores, funciones de perdida y activaci贸n)
```bash
https://github.com/LeonDavisCoropuna/MLP-Multi-Layer-Perceptron.git
```