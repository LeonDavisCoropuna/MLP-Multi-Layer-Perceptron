# CNN - Forward

By Leon Davis.

Este proyecto implementa una red CNN (solo forward) entrenada para reconocer d√≠gitos del 0 al 9 utilizando el dataset MNIST. Se ha desarrollado en C++ usando CMake como sistema de construcci√≥n y OpenCV para el manejo de im√°genes.

## üîß Requisitos

* CMake >= 3.10
* OpenCV >= 4.0

Aseg√∫rate de tener instalados los requisitos antes de compilar.

## Instalaci√≥n

Clona el repositorio y entra en la carpeta del proyecto:

```bash
git clone https://github.com/LeonDavisCoropuna/MLP-Multi-Layer-Perceptron.git
cd MLP-Multi-Layer-Perceptron
```

Dale permisos de ejecuci√≥n al script principal:

```bash
chmod +x run.sh
```

Ejecuta el script para compilar y correr (test o main):

```bash
./run.sh main
```


### 1. Implementaci√≥n
Este documento describe la implementaci√≥n de tres componentes fundamentales para redes neuronales convolucionales:
1. Capa convolucional (`Conv2DLayer`)
2. Capa de pooling m√°ximo (`PoolingLayer`)
3. Capa de aplanamiento (`FlattenLayer`)

### 2. Implementaci√≥n de la Convoluci√≥n 2D

#### 2.1. Estructura de la Capa Convolucional
La clase `Conv2DLayer` implementa la operaci√≥n de convoluci√≥n discreta 2D con las siguientes caracter√≠sticas:
- Soporte para m√∫ltiples canales de entrada y salida
- Inicializaci√≥n de pesos con el m√©todo He normal
- Aplicaci√≥n opcional de funciones de activaci√≥n

```cpp
Conv2DLayer(int in_channels, int out_channels, int kernel_size, 
            int in_height, int in_width, 
            ActivationFunction* act = nullptr, 
            Optimizer* opt = nullptr)
```

#### 2.2. Proceso de Convoluci√≥n (Forward Pass)
El m√©todo `forward` implementa la operaci√≥n principal:

1. **Validaci√≥n de dimensiones**:
   - Verifica que el tensor de entrada tenga las dimensiones esperadas `[in_channels, in_height, in_width]`

2. **Operaci√≥n de convoluci√≥n**:
   - Seis bucles anidados implementan:
     - Canales de salida (co)
     - Posiciones espaciales (h, w)
     - Canales de entrada (ci)
     - Posiciones del kernel (kh, kw)

3. **C√°lculo de cada elemento de salida**:
   ```cpp
   sum += inputs.data[ci*in_height*in_width + input_h*in_width + input_w] * 
          weights.data[co*in_channels*kernel_size*kernel_size + 
                      ci*kernel_size*kernel_size + 
                      kh*kernel_size + kw];
   ```

4. **Aplicaci√≥n de activaci√≥n**:
   ```cpp
   outputs.data[co*out_height*out_width + h*out_width + w] = 
       activation ? activation->activate(sum) : sum;
   ```

#### 2.3. Ejemplo Num√©rico
Para una entrada de 1√ó3√ó3 (1 canal, 3√ó3) y filtro 2√ó2:
```
Entrada: [ [1, 2, 3],
           [4, 5, 6],
           [7, 8, 9] ]
Filtro: [ [0.1, 0.2],
          [0.3, 0.4] ]
Salida: [ [1*0.1 + 2*0.2 + 4*0.3 + 5*0.4, ...],
          ... ]
```

### 3. Implementaci√≥n del Pooling M√°ximo

#### 3.1. Estructura de la Capa de Pooling
La clase `PoolingLayer` implementa pooling m√°ximo con:
- Soporte para tama√±o de kernel configurable
- Stride configurable
- Validaci√≥n estricta de dimensiones

```cpp
PoolingLayer(int channels, int in_height, int in_width, 
             int kernel_size = 2, int stride = 1)
```

#### 3.2. Proceso de Pooling (Forward Pass)
El m√©todo `forward` implementa:

1. **Validaci√≥n de dimensiones**:
   - Comprueba que las dimensiones de entrada sean compatibles con el kernel y stride

2. **Pooling m√°ximo**:
   - Para cada ventana del kernel:
     - Encuentra el valor m√°ximo
     - Guarda el √≠ndice de la posici√≥n m√°xima para backpropagation

3. **Restricciones dimensionales**:
   ```cpp
   if ((in_height - kernel_size) % stride != 0 || 
       (in_width - kernel_size) % stride != 0) {
       throw std::invalid_argument("Incompatible dimensions");
   }
   ```

#### 3.3. Ejemplo Num√©rico
Para entrada 1√ó4√ó4, kernel 2√ó2, stride 2:
```
Entrada: [ [1, 2, 3, 4],
           [5, 6, 7, 8],
           [9,10,11,12],
           [13,14,15,16] ]
Salida: [ [6, 8],
          [14, 16] ]
```

### 4. Implementaci√≥n de Flatten

#### 4.1. Estructura de la Capa
La clase `FlattenLayer` transforma tensores multidimensionales en vectores 1D:

```cpp
Tensor forward(const Tensor &input) {
    input_shape = input.shape;
    outputs = input;
    outputs.reshape({static_cast<int>(input.size())});
    return outputs;
}
```

#### 4.2. Ejemplo
Entrada 2√ó3√ó3 ‚Üí Salida 18√ó1

## Ejemplo 1:

### Descripci√≥n del Ejemplo
Este ejemplo demuestra el procesamiento de una imagen de entrada 5√ó5 a trav√©s de una red neuronal minimalista compuesta por:
1. Una capa convolucional con 2 filtros
2. Una capa de pooling m√°ximo
3. Solo se implementa el forward pass (propagaci√≥n hacia adelante)

### Arquitectura de la Red
```cpp
MLP mlp(0, nullptr);
mlp.add_layer(new Conv2DLayer(1, 2, 3, 5, 5));  // Conv 1‚Üí2 canales, kernel 3√ó3
mlp.add_layer(new PoolingLayer(2, 3, 3, 3, 1));  // Pooling kernel 3√ó3, stride 1
```

### Flujo de Datos
1. **Entrada**: Tensor 1√ó5√ó5 (valores del 1 al 25)
2. **Capa Conv2D**:
   - Transforma 1 canal ‚Üí 2 canales
   - Reduce dimensiones espaciales de 5√ó5 ‚Üí 3√ó3 (kernel 3√ó3 sin padding)
   - Aplica pesos aleatorios inicializados con semilla fija (32)
3. **Capa Pooling**:
   - Operaci√≥n m√°ximo en ventanas 3√ó3
   - Reduce 3√ó3 ‚Üí 1√ó1 por canal
   - Stride=1 permite operaci√≥n sin errores dimensionales

### Resultados Obtenidos
```
Salida final (2x1x1):
2.26604 -0.670125
```

### An√°lisis de Resultados
1. **Dimensionalidad**:
   - La salida tiene tama√±o 2√ó1√ó1 como se esperaba
   - 2 canales (uno por cada filtro convolucional)
   - 1√ó1 por el pooling agresivo

2. **Valores de Salida**:
   - Los valores (2.26604 y -0.670125) son consistentes con:
     - Pesos inicializados aleatoriamente (pero reproducibles por la semilla fija)
     - Operaci√≥n de m√°ximo sobre los feature maps intermedios
   - La diferencia entre canales muestra que los filtros aprenden caracter√≠sticas distintas

## Ejemplo 2: Procesamiento con Flatten para Redes Fully-Connected

### Objetivo
Este ejemplo demuestra c√≥mo integrar una capa Flatten despu√©s de las capas convolucionales para preparar los datos para una red fully-connected, mostrando la transformaci√≥n dimensional completa.

### Arquitectura Propuesta
```cpp
MLP mlp(0, nullptr);

// Bloques Convolucionales
mlp.add_layer(new Conv2DLayer(1, 4, 3, 6, 6));    // 4x4x4 (1x6x6 ‚Üí 4x4x4)
mlp.add_layer(new PoolingLayer(4, 4, 4, 2, 2));   // 4x2x2

// Capa Flatten
mlp.add_layer(new FlattenLayer());                // 4x2x2 ‚Üí 16

// Visualizaci√≥n
Tensor output = mlp.forward(input);
```

### Flujo de Transformaci√≥n Dimensional
1. **Input**: `[1, 6, 6]` (1 canal 6√ó6)
2. **Conv2D**: 
   - 4 filtros 3√ó3 ‚Üí `[4, 4, 4]` 
   - C√°lculo: `(6-3)+1 = 4`
3. **Pooling**: 
   - MaxPool 2√ó2 stride 2 ‚Üí `[4, 2, 2]`
   - C√°lculo: `‚åä(4-2)/2‚åã+1 = 2`
4. **Flatten**: 
   - `[4, 2, 2]` ‚Üí `[16]` (4√ó2√ó2=16)

### Resultado Esperado
![alt text](images/test-2-cnn.png)

### An√°lisis Clave
1. **Preservaci√≥n de Informaci√≥n**:
   - El patr√≥n diagonal de entrada se codifica en 16 caracter√≠sticas
   - Flatten mantiene la localidad espacial original (primeros 4 valores = esquina superior izquierda)

2. **Uso en Redes Completas**:
   ```cpp
   // Ejemplo de continuaci√≥n para clasificaci√≥n
   mlp.add_layer(new DenseLayer(16, 10, new Softmax()));
   ```

## Intento de entrenamiento

![alt text](images/cnn-backward.png)
Si llega a entrenar pero la mejora es muy poca y costosa.

## 5. Conclusiones

1. Se implement√≥ exitosamente la convoluci√≥n 2D con soporte para:
   - M√∫ltiples filtros (canales de salida)
   - Validaci√≥n dimensional estricta
   - Inicializaci√≥n adecuada de pesos

2. El pooling m√°ximo incluye:
   - Configuraci√≥n flexible de kernel y stride
   - Mecanismo para backpropagation (√≠ndices m√°ximos)
   - Validaci√≥n de compatibilidad dimensional

3. El c√≥digo cumple con los requisitos solicitados:
   - Operaciones b√°sicas de CNN
   - Manejo adecuado de dimensiones
   - Estructura clara y documentada


## Ver c√≥digo en github:
La parte principal del c√≥digo se encuentra en la carpeta models/ (MLP, layers) y en utils/ (optimizadores, funciones de perdida y activaci√≥n)
```bash
https://github.com/LeonDavisCoropuna/MLP-Multi-Layer-Perceptron.git
```